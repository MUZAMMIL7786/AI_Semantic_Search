# -*- coding: utf-8 -*-
"""AI_Semantic_Search.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1R_8FHQEKo-wJw4l9f86clkdgcPQSKteb

# Pre-requisites
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import scipy

pip install langchain --upgrade

pip install openai

from langchain.text_splitter import RecursiveCharacterTextSplitter

"""# User Interface"""

pip install gradio

import gradio as gr

pip install pypdf

from langchain.document_loaders import PyPDFLoader

# loader = PyPDFLoader("field-guide-to-data-science.pdf")
# data = loader.load_and_split()

import gradio as gr
data1 = []

def greet(Query, Custom_search):
    documents = search_query(Query, Custom_search)
    observations = entity_recog(documents)
    return documents, observations

def upload_file(file):
    print(file)
    loader = PyPDFLoader(file.name)
    data1.append(loader.load_and_split())
    return file.name

with gr.Blocks() as demo:
    
    file_output = gr.File()
    upload_button = gr.UploadButton("Click to Upload a File")
    upload_button.upload(upload_file, upload_button, file_output)

    input1 = gr.Textbox(label="Query")
    input2 = gr.Textbox(label="Customizable Search")
    output1 = gr.Textbox(label="Query Search Results")
    output2 = gr.DataFrame(label = "Entity Results")
    
    greet_btn = gr.Button("Search")
    greet_btn.click(fn=greet, inputs=[input1, input2], outputs=[output1, output2])
    # gr_btn.click(fn=greet, inputs=["text", "text"], outputs=["text", gr.DataFrame()])

demo.launch(share = True)

data = data1[0]
data

# !pip freeze

"""# Load Data"""

print (f'You have {len(data)} document(s) in your data')
print (f'There are {len(data[0].page_content)} characters in your document')

text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=0)
texts = text_splitter.split_documents(data)

print (f'Now you have {len(texts)} documents')

data_science = pd.DataFrame(texts, columns = ['text', 'metadata'])
data_science['text'] = data_science['text'].str.get(1)

data = data_science['text']
len(data)

"""# **Exploratoty Data Analysis**"""

data_science['num_char'] = data_science['text'].apply(len)
data_science.head()

import nltk
nltk.download('punkt')

data_science['num_words'] = data_science['text'].apply(lambda x: len(nltk.word_tokenize(x)))
data_science.head()

data_science['num_sent'] = data_science['text'].apply(lambda x: len(nltk.sent_tokenize(x)))
data_science.head()

data_science[['num_char', 'num_words', 'num_sent']].describe()

"""# **Text Preprocessing**

1. Lower case
2. Tokenization
3. Removing special characters
4. Removing stop words and punctuations
5. stemming - dance, dancing, -> dance

"""

from nltk.corpus import stopwords
import string
from nltk.stem.porter import PorterStemmer
ps = PorterStemmer()

def text_preprocessing(text):
  text = text.lower()
  text = nltk.word_tokenize(text)

  y = []
  for i in text:
    if i.isalnum():
      y.append(i)

  text = y.copy()
  y = []
  for i in text:
    if i not in stopwords.words('english') and i not in string.punctuation:
      y.append(i)

  text = y.copy()
  y = []
  for i in text:
    y.append(ps.stem(i))

  return ' '.join(y)

import nltk
nltk.download('stopwords')

text_preprocessing('I am enjoying Machine Learning projects. what about you?')

data_science['tranformed_text'] = data_science['text'].apply(text_preprocessing)
data_science.head()

from wordcloud import WordCloud

wc = WordCloud(width = 500, height = 500, min_font_size = 10, background_color = 'black')

ds_wc = wc.generate(data_science['tranformed_text'].str.cat(sep = ' '))

plt.figure(figsize = (12, 6))
plt.imshow(ds_wc)

"""# Load Data for semantic search"""

# Adding Multilingual

data = data_science['text'].tolist()
data.append("مرحباً كيف حالك؟")

print('documents -->', len(data))

"""# Vector Database"""

pip install chromadb

from langchain.vectorstores import Chroma
from langchain.embeddings.openai import OpenAIEmbeddings
from chromadb.utils import embedding_functions

data_id = [str(i) for i in range(1, len(data)+1)]
texts = pd.DataFrame(data, index = data_id, columns = ['text'])
texts.head()

"""# Vectorization Algorithm"""

import chromadb
from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction

# We initialize an embedding function, and provide it to the collection.
embedding_function = OpenAIEmbeddingFunction(api_key="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")

chroma_client = chromadb.Client()
openai_ef_coll = chroma_client.create_collection(name='openai_ef', embedding_function=embedding_function)

batch_size = 100

for i in range(0, len(data), batch_size):

    batch_id = data_id[i:i+batch_size]
    batch_df = data[i:i+batch_size]  
    openai_ef_coll.add(
        
        ids = batch_id, # Chroma takes string IDs.
        documents= batch_df, 
    )

openai_ef_coll

"""# Similarity Search Algorithm"""

import spacy
nlp = spacy.load('en_core_web_sm')

def text_preprocessing(text):
  text = text.lower()
  text = nltk.word_tokenize(text)

  y = []
  for i in text:
    if i.isalnum():
      y.append(i)

  text = y.copy()
  y = []
  for i in text:
    if i not in string.punctuation:
      y.append(i)


  return ' '.join(y)

def entity_recog(documents):

  data = []

  for doc in documents.split('\n'):

    doc = nlp(doc)
    if doc.ents:
      for ent in doc.ents:
        obj = {}
        obj['ent_text'] = ent.text
        obj['ent.label'] = ent.label_
        obj['ent_obj'] = str(spacy.explain(ent.label_))
        # print(ent.text + ' - ' + ent.label_+ ' - ' + str(spacy.explain(ent.label_)) )
        data.append(obj)

  df = pd.DataFrame(data)
  df.drop_duplicates(inplace = True)
  return df

entity_recog("I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today.")

def search_query(Query, Custom_search):
    
    query_result = ''
    if Query != "":
      doc = openai_ef_coll.query(
        query_texts = [Query],
        where_document={"$contains":Custom_search},
        n_results = 5
      )
      
      for idx, txt, dis in zip(doc['ids'][0], doc['documents'][0], doc['distances'][0]):
        query_result += str(idx)+ '. ' + text_preprocessing(txt) + ' (' + str(100 - dis *100)[:4] + '%' + ')' + '\n'

    return query_result

Query = "what is data science"; Custom_search = ""
documents = search_query(Query, Custom_search)
print(documents)

!pip freeze